<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solvers · JSOSolvers.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="JSOSolvers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">JSOSolvers.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Solvers</a><ul class="internal"><li><a class="tocitem" href="#Solver-list"><span>Solver list</span></a></li></ul></li><li><a class="tocitem" href="../internal/">Internal</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/master/docs/src/solvers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Solvers"><a class="docs-heading-anchor" href="#Solvers">Solvers</a><a id="Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Solvers" title="Permalink"></a></h1><p><strong>Solver list</strong></p><ul><li><a href="#JSOSolvers.lbfgs"><code>lbfgs</code></a></li><li><a href="#JSOSolvers.tron"><code>tron</code></a></li><li><a href="#JSOSolvers.trunk"><code>trunk</code></a></li><li><a href="#JSOSolvers.R2"><code>R2</code></a></li></ul><table><tr><th style="text-align: right">Problem type</th><th style="text-align: right">Solvers</th></tr><tr><td style="text-align: right">Unconstrained NLP</td><td style="text-align: right"><a href="#JSOSolvers.lbfgs"><code>lbfgs</code></a>, <a href="#JSOSolvers.tron"><code>tron</code></a>, <a href="#JSOSolvers.trunk"><code>trunk</code></a>, <a href="#JSOSolvers.R2"><code>R2</code></a></td></tr><tr><td style="text-align: right">Unconstrained NLS</td><td style="text-align: right"><a href="#JSOSolvers.trunk"><code>trunk</code></a>, <a href="#JSOSolvers.tron"><code>tron</code></a></td></tr><tr><td style="text-align: right">Bound-constrained NLP</td><td style="text-align: right"><a href="#JSOSolvers.tron"><code>tron</code></a></td></tr><tr><td style="text-align: right">Bound-constrained NLS</td><td style="text-align: right"><a href="#JSOSolvers.tron"><code>tron</code></a></td></tr></table><h2 id="Solver-list"><a class="docs-heading-anchor" href="#Solver-list">Solver list</a><a id="Solver-list-1"></a><a class="docs-heading-anchor-permalink" href="#Solver-list" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.lbfgs" href="#JSOSolvers.lbfgs"><code>JSOSolvers.lbfgs</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="language-none">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: memory parameter of the <code>lbfgs</code> algorithm.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: slope factor in the Wolfe condition when performing the line search.</li><li><code>bk_max:: Int = 25</code>: maximum number of backtracks when performing the line search.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/lbfgs.jl#L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.tron" href="#JSOSolvers.tron"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="language-none">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/tron.jl#L172">source</a></section><section><div><pre><code class="language-julia">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver::Symbol = :lsmr</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/tronls.jl#L145">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.trunk" href="#JSOSolvers.trunk"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolver(nlp, subsolver_type::Type{&lt;:KrylovSolver} = CgSolver)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/trunk.jl#L134">source</a></section><section><div><pre><code class="language-julia">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolverNLS(nls, subsolver_type::Type{&lt;:KrylovSolver} = LsmrSolver)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available <code>KrylovSolver</code>.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/trunkls.jl#L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.R2" href="#JSOSolvers.R2"><code>JSOSolvers.R2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">R2(nlp; kwargs...)</code></pre><p>A first-order quadratic regularization method for unconstrained optimization.</p><p>For advanced usage, first define a <code>R2Solver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = R2Solver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1/4)</code>, <code>η2 = T(0.95)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = 1/γ1</code>: regularization update parameters.</li><li><code>σmin = eps(T)</code>: step parameter for R2 algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(0) ∈ [0,1]</code> is the constant in the momentum term. If <code>β == 0</code>, R2 does not use momentum.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = R2Solver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/a5c2184e94b7a77c0fb4c65e89af8b6f55787e16/src/R2.jl#L85">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../internal/">Internal »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 5 June 2023 20:47">Monday 5 June 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
