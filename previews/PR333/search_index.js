var documenterSearchIndex = {"docs":
[{"location":"performance-tips/#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"The main functions have an in-place variant that allows one to solve multiple optimization problems with the same dimensions and data types.\n\nMore general solvers can take advantage of this functionality by allocating workspace for the solve only once. The in-place variants only require a Julia structure that contains all the storage needed as an additional argument. In-place methods limit memory allocations and deallocations, which are particularly expensive on GPUs.\n\nusing NLPModels, NLPModelsTest\nnlp = BROWNDEN() # test problem from NLPModelsTest with allocation free evaluations\n\nusing SolverCore\nstats = GenericExecutionStats(nlp) # pre-allocate output structure\nusing JSOSolvers\nsolver = TronSolver(nlp) # pre-allocate workspace\nSolverCore.solve!(solver, nlp, stats) # call tron in place\nJSOSolvers.reset!(solver)\nNLPModels.reset!(nlp) # reset counters\n(@allocated SolverCore.solve!(solver, nlp, stats)) == 0\n","category":"section"},{"location":"examples/#Examples","page":"-","title":"Examples","text":"Beyond this repository's documentation, you can also find a list of tutorials on JuliaSmoothOptimizers Tutorials by selecting the tag JSOSolvers.jl. For instance, the tutorial Introduction to JSOSolvers is a good starting point.","category":"section"},{"location":"solvers/#Solvers","page":"Solvers","title":"Solvers","text":"Solver list\n\nlbfgs\ntron\ntrunk\nR2\nfomo\n\nProblem type Solvers\nUnconstrained NLP lbfgs, tron, trunk, R2, fomo\nUnconstrained NLS trunk, tron\nBound-constrained NLP tron\nBound-constrained NLS tron","category":"section"},{"location":"solvers/#Solver-list","page":"Solvers","title":"Solver list","text":"","category":"section"},{"location":"solvers/#JSOSolvers.lbfgs","page":"Solvers","title":"JSOSolvers.lbfgs","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: algorithm parameter, see LBFGSParameterSet.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): algorithm parameter, see LBFGSParameterSet.\nbk_max:: Int = 25: algorithm parameter, see LBFGSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.tron","page":"Solvers","title":"JSOSolvers.tron","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONParameterSet.\nσ::T = T(10): algorithm parameter, see TRONParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\ntron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver::Symbol=:lsmr; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver::Symbol = :lsmr: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONLSParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONLSParameterSet.\nσ::T = T(10): algorithm parameter, see TRONLSParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.trunk","page":"Solvers","title":"JSOSolvers.trunk","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver::Symbol = :cg)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKParameterSet.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\nM: linear operator that models a Hermitian positive-definite matrix of size n; passed to Krylov subsolvers. \n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\ntrunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver::Symbol = :lsmr)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKLSParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKLSParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKLSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available Krylov solvers.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.R2","page":"Solvers","title":"JSOSolvers.R2","text":"fo(nlp; kwargs...)\nR2(nlp; kwargs...)\nTR(nlp; kwargs...)\n\nA First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nR2 and TR runs fo with the dedicated step_backend keyword argument.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4): algorithm parameter, see FOMOParameterSet.\nη2 = T(95/100): algorithm parameter, see FOMOParameterSet.\nγ1 = T(1/2): algorithm parameter, see FOMOParameterSet.\nγ2 = T(2): algorithm parameter, see FOMOParameterSet.\nαmax = 1/eps(T): algorithm parameter, see FOMOParameterSet.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nM = 1 : algorithm parameter, see FOMOParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): algorithm parameter, see FOMOParameterSet.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.fomo","page":"Solvers","title":"JSOSolvers.fomo","text":"fomo(nlp; kwargs...)\n\nA First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nAlgorithm description\n\nThe step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) . ∇f(xk) + βmax . mk‖       (2) In the nonmonotone case, (1) rewrites (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk + (fm - fk)/μk ≥ θ1 * ‖∇f(xk)‖², with fm the largest objective value over the last M successful iterations, and fk = f(xk).\n\nAdvanced usage\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FomoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nNo momentum: if the user does not whish to use momentum (β = 0), it is recommended to use the memory-optimized fo method.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4), η2 = T(95/100): step acceptance parameters.\nγ1 = T(1/2), γ2 = T(2): regularization update parameters.\nγ3 = T(1/2) : momentum factor βmax update parameter in case of unsuccessful iteration.\nαmax = 1/eps(T): maximum step parameter for fomo algorithm.\nmax_eval::Int = -1: maximum number of objective evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(9/10) ∈ [0,1): target decay rate for the momentum.\nθ1 = T(1/10): momentum contribution parameter for convergence condition (1).\nθ2 = eps(T)^(1/3): momentum contribution parameter for convergence condition (2). \nM = 1 : requires objective decrease over the M last iterates (nonmonotone context). M=1 implies monotone behaviour. \nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): step computation mode. Options are r2_step() for quadratic regulation step and tr_step() for first-order trust-region.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user || stats.stats = :unknown will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nfomo\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fomo(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FomoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"function"},{"location":"internal/#Internal-functions","page":"Internal functions","title":"Internal functions","text":"","category":"section"},{"location":"internal/#JSOSolvers.projected_newton!","page":"Internal functions","title":"JSOSolvers.projected_newton!","text":"projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_time = Inf, max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_line_search!","page":"Internal functions","title":"JSOSolvers.projected_line_search!","text":"s = projected_line_search!(x, H, g, d, ℓ, u, Hs, μ₀)\n\nPerforms a projected line search, searching for a step size t such that\n\n0.5sᵀHs + sᵀg ≦ μ₀sᵀg,\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.cauchy!","page":"Internal functions","title":"JSOSolvers.cauchy!","text":"α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀sᵀg.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_gauss_newton!","page":"Internal functions","title":"JSOSolvers.projected_gauss_newton!","text":"projected_gauss_newton!(solver, x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u; max_cgiter = 50, max_time = Inf, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_line_search_ls!","page":"Internal functions","title":"JSOSolvers.projected_line_search_ls!","text":"s = projected_line_search_ls!(x, A, g, d, ℓ, u, As, s; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.cauchy_ls!","page":"Internal functions","title":"JSOSolvers.cauchy_ls!","text":"α, s = cauchy_ls!(x, A, Fx, g, Δ, ℓ, u, s, As; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀gᵀs,\n\nwhere g = AᵀFx.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"​","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"​\n\nPages = [\"reference.md\"]\n\n​","category":"section"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"​\n\nPages = [\"reference.md\"]\n\n​","category":"section"},{"location":"reference/#JSOSolvers.FOMOParameterSet","page":"Reference","title":"JSOSolvers.FOMOParameterSet","text":"FOMOParameterSet{T} <: AbstractParameterSet\n\nThis structure designed for fomo regroups the following parameters:\n\nη1, η2: step acceptance parameters.\nγ1, γ2: regularization update parameters.\nγ3 : momentum factor βmax update parameter in case of unsuccessful iteration.\nαmax: maximum step parameter for fomo algorithm.\nβ ∈ [0,1): target decay rate for the momentum.\nθ1: momentum contribution parameter for convergence condition (1).\nθ2: momentum contribution parameter for convergence condition (2). \nM : requires objective decrease over the M last iterates (nonmonotone context). M=1 implies monotone behaviour. \nstep_backend: step computation mode. Options are r2_step() for quadratic regulation step and tr_step() for first-order trust-region.\n\nAn additional constructor is\n\nFOMOParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nη1::T = eps(T)^(1 // 4)\nη2::T = T(95/100)\nγ1::T = T(1/2)\nγ2::T = T(2)\nγ3::T = T(1/2)\nαmax::T = 1/eps(T)\nβ = T(9/10) ∈ [0,1)\nθ1 = T(1/10)\nθ2 = eps(T)^(1/3)\nM = 1\n`stepbackend = r2step()\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.FoSolver","page":"Reference","title":"JSOSolvers.FoSolver","text":"fo(nlp; kwargs...)\nR2(nlp; kwargs...)\nTR(nlp; kwargs...)\n\nA First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nR2 and TR runs fo with the dedicated step_backend keyword argument.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4): algorithm parameter, see FOMOParameterSet.\nη2 = T(95/100): algorithm parameter, see FOMOParameterSet.\nγ1 = T(1/2): algorithm parameter, see FOMOParameterSet.\nγ2 = T(2): algorithm parameter, see FOMOParameterSet.\nαmax = 1/eps(T): algorithm parameter, see FOMOParameterSet.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nM = 1 : algorithm parameter, see FOMOParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): algorithm parameter, see FOMOParameterSet.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.FomoSolver","page":"Reference","title":"JSOSolvers.FomoSolver","text":"fomo(nlp; kwargs...)\n\nA First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nAlgorithm description\n\nThe step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) . ∇f(xk) + βmax . mk‖       (2) In the nonmonotone case, (1) rewrites (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk + (fm - fk)/μk ≥ θ1 * ‖∇f(xk)‖², with fm the largest objective value over the last M successful iterations, and fk = f(xk).\n\nAdvanced usage\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FomoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nNo momentum: if the user does not whish to use momentum (β = 0), it is recommended to use the memory-optimized fo method.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4), η2 = T(95/100): step acceptance parameters.\nγ1 = T(1/2), γ2 = T(2): regularization update parameters.\nγ3 = T(1/2) : momentum factor βmax update parameter in case of unsuccessful iteration.\nαmax = 1/eps(T): maximum step parameter for fomo algorithm.\nmax_eval::Int = -1: maximum number of objective evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(9/10) ∈ [0,1): target decay rate for the momentum.\nθ1 = T(1/10): momentum contribution parameter for convergence condition (1).\nθ2 = eps(T)^(1/3): momentum contribution parameter for convergence condition (2). \nM = 1 : requires objective decrease over the M last iterates (nonmonotone context). M=1 implies monotone behaviour. \nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): step computation mode. Options are r2_step() for quadratic regulation step and tr_step() for first-order trust-region.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user || stats.stats = :unknown will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nfomo\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fomo(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FomoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.LBFGSParameterSet","page":"Reference","title":"JSOSolvers.LBFGSParameterSet","text":"LBFGSParameterSet{T} <: AbstractParameterSet\n\nThis structure designed for lbfgs regroups the following parameters:\n\nmem: memory parameter of the lbfgs algorithm\nτ₁: slope factor in the Wolfe condition when performing the line search\nbk_max: maximum number of backtracks when performing the line search.\n\nAn additional constructor is\n\nLBFGSParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nmem::Int = 5\nτ₁::T = T(0.9999)\nbk_max:: Int = 25\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.LBFGSSolver","page":"Reference","title":"JSOSolvers.LBFGSSolver","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: algorithm parameter, see LBFGSParameterSet.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): algorithm parameter, see LBFGSParameterSet.\nbk_max:: Int = 25: algorithm parameter, see LBFGSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.R2Solver","page":"Reference","title":"JSOSolvers.R2Solver","text":"`R2Solver` is deprecated, please check the documentation of `R2`.\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TRONLSParameterSet","page":"Reference","title":"JSOSolvers.TRONLSParameterSet","text":"TRONLSParameterSet{T} <: AbstractParameterSet\n\nThis structure designed for tron regroups the following parameters:\n\nμ₀: algorithm parameter in (0, 0.5).\nμ₁: algorithm parameter in (0, +∞).\nσ: algorithm parameter in (1, +∞).\n\nAn additional constructor is\n\nTRONLSParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nμ₀::T = T(1 / 100)\nμ₁::T = T(1)\nσ::T = T(10)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TRONParameterSet","page":"Reference","title":"JSOSolvers.TRONParameterSet","text":"TRONParameterSet{T} <: AbstractParameterSet\n\nThis structure designed for tron regroups the following parameters:\n\nμ₀::T: algorithm parameter in (0, 0.5).\nμ₁::T: algorithm parameter in (0, +∞).\nσ::T: algorithm parameter in (1, +∞).\n\nAn additional constructor is\n\nTRONParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nμ₀::T = T(1 / 100)\nμ₁::T = T(1)\nσ::T = T(10)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TRUNKLSParameterSet","page":"Reference","title":"JSOSolvers.TRUNKLSParameterSet","text":"TRUNKLSParameterSet <: AbstractParameterSet\n\nThis structure designed for tron regroups the following parameters:\n\nbk_max: algorithm parameter.\nmonotone: algorithm parameter.\nnm_itmax: algorithm parameter.\n\nAn additional constructor is\n\nTRUNKLSParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nbk_max::Int = 10\nmonotone::Bool = true\nnm_itmax::Int = 25\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TRUNKParameterSet","page":"Reference","title":"JSOSolvers.TRUNKParameterSet","text":"TRUNKParameterSet <: AbstractParameterSet\n\nThis structure designed for tron regroups the following parameters:\n\nbk_max: algorithm parameter.\nmonotone: algorithm parameter.\nnm_itmax: algorithm parameter.\n\nAn additional constructor is\n\nTRUNKParameterSet(nlp: kwargs...)\n\nwhere the kwargs are the parameters above.\n\nDefault values are:\n\nbk_max::Int = 10\nmonotone::Bool = true\nnm_itmax::Int = 25\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TronSolver","page":"Reference","title":"JSOSolvers.TronSolver","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONParameterSet.\nσ::T = T(10): algorithm parameter, see TRONParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TronSolverNLS","page":"Reference","title":"JSOSolvers.TronSolverNLS","text":"tron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver::Symbol=:lsmr; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver::Symbol = :lsmr: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONLSParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONLSParameterSet.\nσ::T = T(10): algorithm parameter, see TRONLSParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TrunkSolver","page":"Reference","title":"JSOSolvers.TrunkSolver","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver::Symbol = :cg)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKParameterSet.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\nM: linear operator that models a Hermitian positive-definite matrix of size n; passed to Krylov subsolvers. \n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TrunkSolverNLS","page":"Reference","title":"JSOSolvers.TrunkSolverNLS","text":"trunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver::Symbol = :lsmr)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKLSParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKLSParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKLSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available Krylov solvers.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.R2","text":"fo(nlp; kwargs...)\nR2(nlp; kwargs...)\nTR(nlp; kwargs...)\n\nA First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nR2 and TR runs fo with the dedicated step_backend keyword argument.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4): algorithm parameter, see FOMOParameterSet.\nη2 = T(95/100): algorithm parameter, see FOMOParameterSet.\nγ1 = T(1/2): algorithm parameter, see FOMOParameterSet.\nγ2 = T(2): algorithm parameter, see FOMOParameterSet.\nαmax = 1/eps(T): algorithm parameter, see FOMOParameterSet.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nM = 1 : algorithm parameter, see FOMOParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): algorithm parameter, see FOMOParameterSet.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.TR-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.TR","text":"fo(nlp; kwargs...)\nR2(nlp; kwargs...)\nTR(nlp; kwargs...)\n\nA First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nR2 and TR runs fo with the dedicated step_backend keyword argument.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4): algorithm parameter, see FOMOParameterSet.\nη2 = T(95/100): algorithm parameter, see FOMOParameterSet.\nγ1 = T(1/2): algorithm parameter, see FOMOParameterSet.\nγ2 = T(2): algorithm parameter, see FOMOParameterSet.\nαmax = 1/eps(T): algorithm parameter, see FOMOParameterSet.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nM = 1 : algorithm parameter, see FOMOParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): algorithm parameter, see FOMOParameterSet.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.cauchy!","text":"α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀sᵀg.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.cauchy_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.cauchy_ls!","text":"α, s = cauchy_ls!(x, A, Fx, g, Δ, ℓ, u, s, As; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀gᵀs,\n\nwhere g = AᵀFx.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.find_beta-Union{Tuple{V}, Tuple{T}, Tuple{V, Vararg{T, 8}}} where {T, V}","page":"Reference","title":"JSOSolvers.find_beta","text":"find_beta(m, mdot∇f, norm_∇f, μk, fk, max_obj_mem, β, θ1, θ2)\n\nCompute βmax which saturates the contribution of the momentum term to the gradient. βmax is computed such that the two gradient-related conditions (first one is relaxed in the nonmonotone case) are ensured: \n\n(1-βmax) * ‖∇f(xk)‖² + βmax * ∇f(xk)ᵀm + (maxobjmem - fk)/μk ≥ θ1 * ‖∇f(xk)‖²\n‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) * ∇f(xk) .+ βmax .* m‖\n\nwith m the momentum term and mdot∇f = ∇f(xk)ᵀm, fk the model at s=0, max_obj_mem the largest objective value over the last M successful iterations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.fo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.fo","text":"fo(nlp; kwargs...)\nR2(nlp; kwargs...)\nTR(nlp; kwargs...)\n\nA First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nR2 and TR runs fo with the dedicated step_backend keyword argument.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4): algorithm parameter, see FOMOParameterSet.\nη2 = T(95/100): algorithm parameter, see FOMOParameterSet.\nγ1 = T(1/2): algorithm parameter, see FOMOParameterSet.\nγ2 = T(2): algorithm parameter, see FOMOParameterSet.\nαmax = 1/eps(T): algorithm parameter, see FOMOParameterSet.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nM = 1 : algorithm parameter, see FOMOParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): algorithm parameter, see FOMOParameterSet.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.fomo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.fomo","text":"fomo(nlp; kwargs...)\n\nA First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.\n\nAlgorithm description\n\nThe step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) . ∇f(xk) + βmax . mk‖       (2) In the nonmonotone case, (1) rewrites (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk + (fm - fk)/μk ≥ θ1 * ‖∇f(xk)‖², with fm the largest objective value over the last M successful iterations, and fk = f(xk).\n\nAdvanced usage\n\nFor advanced usage, first define a FomoSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = FomoSolver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nNo momentum: if the user does not whish to use momentum (β = 0), it is recommended to use the memory-optimized fo method.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1 // 4), η2 = T(95/100): step acceptance parameters.\nγ1 = T(1/2), γ2 = T(2): regularization update parameters.\nγ3 = T(1/2) : momentum factor βmax update parameter in case of unsuccessful iteration.\nαmax = 1/eps(T): maximum step parameter for fomo algorithm.\nmax_eval::Int = -1: maximum number of objective evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(9/10) ∈ [0,1): target decay rate for the momentum.\nθ1 = T(1/10): momentum contribution parameter for convergence condition (1).\nθ2 = eps(T)^(1/3): momentum contribution parameter for convergence condition (2). \nM = 1 : requires objective decrease over the M last iterates (nonmonotone context). M=1 implies monotone behaviour. \nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nstep_backend = r2_step(): step computation mode. Options are r2_step() for quadratic regulation step and tr_step() for first-order trust-region.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user || stats.stats = :unknown will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nfomo\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = fomo(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = FomoSolver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.init_alpha-Union{Tuple{T}, Tuple{T, r2_step}} where T","page":"Reference","title":"JSOSolvers.init_alpha","text":"init_alpha(norm_∇fk::T, ::r2_step)\ninit_alpha(norm_∇fk::T, ::tr_step)\n\nInitialize α step size parameter. Ensure first step is the same for quadratic regularization and trust region methods.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.lbfgs","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: algorithm parameter, see LBFGSParameterSet.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): algorithm parameter, see LBFGSParameterSet.\nbk_max:: Int = 25: algorithm parameter, see LBFGSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.normM!-NTuple{4, Any}","page":"Reference","title":"JSOSolvers.normM!","text":"normM!(n, x, M, z)\n\nWeighted norm of x with respect to M, i.e., z = sqrt(x' * M * x). Uses z as workspace.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{TronSolverNLS{T, V, Sub, Op, Aop} where {V<:AbstractVector{T}, Sub<:Krylov.KrylovWorkspace{T, T, V}, Op<:LinearOperators.AbstractLinearOperator{T}, Aop<:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_gauss_newton!","text":"projected_gauss_newton!(solver, x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u; max_cgiter = 50, max_time = Inf, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, Real}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_line_search!","text":"s = projected_line_search!(x, H, g, d, ℓ, u, Hs, μ₀)\n\nPerforms a projected line search, searching for a step size t such that\n\n0.5sᵀHs + sᵀg ≦ μ₀sᵀg,\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_line_search_ls!","text":"s = projected_line_search_ls!(x, A, g, d, ℓ, u, As, s; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Sub, Op, Aop} where {V<:AbstractVector{T}, Sub<:Krylov.KrylovWorkspace{T, T, V}, Op<:LinearOperators.AbstractLinearOperator{T}, Aop<:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_newton!","text":"projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_time = Inf, max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.step_mult-Union{Tuple{T}, Tuple{T, T, r2_step}} where T","page":"Reference","title":"JSOSolvers.step_mult","text":"step_mult(α::T, norm_∇fk::T, ::r2_step)\nstep_mult(α::T, norm_∇fk::T, ::tr_step)\n\nCompute step size multiplier: α for quadratic regularization(::r2 and ::R2og) and α/norm_∇fk for trust region (::tr).\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}","page":"Reference","title":"JSOSolvers.tron","text":"tron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver::Symbol=:lsmr; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver::Symbol = :lsmr: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONLSParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONLSParameterSet.\nσ::T = T(10): algorithm parameter, see TRONLSParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}","page":"Reference","title":"JSOSolvers.tron","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1 / 100): algorithm parameter, see TRONParameterSet.\nμ₁::T = T(1): algorithm parameter, see TRONParameterSet.\nσ::T = T(10): algorithm parameter, see TRONParameterSet.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V","page":"Reference","title":"JSOSolvers.trunk","text":"trunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver::Symbol = :lsmr)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKLSParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKLSParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKLSParameterSet.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available Krylov solvers.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V","page":"Reference","title":"JSOSolvers.trunk","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver::Symbol = :cg)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter, see TRUNKParameterSet.\nmonotone::Bool = true: algorithm parameter, see TRUNKParameterSet.\nnm_itmax::Int = 25: algorithm parameter, see TRUNKParameterSet.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\nM: linear operator that models a Hermitian positive-definite matrix of size n; passed to Krylov subsolvers. \n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of the residual, for instance, the norm of the gradient for unconstrained problems;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"method"},{"location":"benchmark/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmark/#CUTEst-benchmark","page":"Benchmarks","title":"CUTEst benchmark","text":"With JSO-compliant solvers, we can run the solver on a set of problems, explore the results, and compare to other JSO-compliant solvers using specialized benchmark tools. We are following here the tutorial in SolverBenchmark.jl to run benchmarks on JSO-compliant solvers.\n\nusing CUTEst\n\nTo test the implementation on bound-constrained problems, we use the package CUTEst.jl, which implements CUTEstModel an instance of AbstractNLPModel.\n\nusing SolverBenchmark\n\nLet us select bound-constrained problems from CUTEst with a maximum of 300 variables.\n\nnmax = 300\npnames_unconstrained = CUTEst.select_sif_problems(\n  max_con = 0,\n  only_free_var = true, # unconstrained\n  max_var = nmax,\n  objtype = 3:6,\n)\npnames = CUTEst.select_sif_problems(\n  max_con = 0,\n  max_var = nmax,\n  objtype = 3:6,\n)\n\npnames = setdiff(pnames, pnames_unconstrained)\ncutest_problems = (CUTEstModel(p) for p in pnames)\n\nlength(cutest_problems) # number of problems\n\nWe compare here TRON from JSOSolvers.jl with Ipopt (Wächter, A., & Biegler, L. T. (2006). On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming. Mathematical programming, 106(1), 25-57.), via the NLPModelsIpopt.jl thin wrapper, on a subset of CUTEst problems.\n\nusing JSOSolvers, NLPModelsIpopt\n\nTo make stopping conditions comparable, we set Ipopt's parameters dual_inf_tol=Inf, constr_viol_tol=Inf and compl_inf_tol=Inf to disable additional stopping conditions related to those tolerances, acceptable_iter=0 to disable the search for an acceptable point.\n\n#Same time limit for all the solvers\nmax_time = 1200. #20 minutes\ntol = 1e-5\n\nsolvers = Dict(\n  :ipopt => nlp -> ipopt(\n    nlp,\n    print_level = 0,\n    dual_inf_tol = Inf,\n    constr_viol_tol = Inf,\n    compl_inf_tol = Inf,\n    acceptable_iter = 0,\n    max_cpu_time = max_time,\n    x0 = nlp.meta.x0,\n    tol = tol,\n  ),\n  :tron => nlp -> tron(\n    nlp,\n    max_time = max_time,\n    max_iter = typemax(Int64),\n    max_eval = typemax(Int64),\n    atol = tol,\n    rtol = tol,\n  ),\n)\n\nstats = bmark_solvers(solvers, cutest_problems)\n\nThe function bmark_solvers return a Dict of DataFrames with detailed information on the execution. This output can be saved in a data file.\n\nusing JLD2\n@save \"ipopt_dcildl_$(string(length(pnames))).jld2\" stats\n\nThe result of the benchmark can be explored via tables,\n\npretty_stats(stats[:tron])\n\nor it can also be used to make performance profiles.\n\nusing Plots\ngr()\n\nlegend = Dict(\n  :neval_obj => \"number of f evals\",\n  :neval_grad => \"number of ∇f evals\",\n  :neval_hprod  => \"number of ∇²f*v evals\",\n  :neval_hess  => \"number of ∇²f evals\",\n  :elapsed_time => \"elapsed time\"\n)\nperf_title(col) = \"Performance profile on CUTEst w.r.t. $(string(legend[col]))\"\n\nstyles = [:solid,:dash,:dot,:dashdot] #[:auto, :solid, :dash, :dot, :dashdot, :dashdotdot]\n\nfunction print_pp_column(col::Symbol, stats)\n\n  ϵ = minimum(minimum(filter(x -> x > 0, df[!, col])) for df in values(stats))\n  first_order(df) = df.status .== :first_order\n  unbounded(df) = df.status .== :unbounded\n  solved(df) = first_order(df) .| unbounded(df)\n  cost(df) = (max.(df[!, col], ϵ) + .!solved(df) .* Inf)\n\n  p = performance_profile(\n    stats,\n    cost,\n    title=perf_title(col),\n    legend=:bottomright,\n    linestyles=styles\n  )\nend\n\nprint_pp_column(:elapsed_time, stats) # with respect to time\n\nprint_pp_column(:neval_obj, stats) # with respect to number of objective evaluations\n\nprint_pp_column(:neval_grad, stats) # with respect to number of gradient evaluations","category":"section"},{"location":"floating-point-systems/#Multiple-Floating-Point-Systems-Support","page":"Multiple Floating-Point Systems Support","title":"Multiple Floating-Point Systems Support","text":"The following example also illustrates that the solvers are compatible with most data structures supported by Julia by running the solver TRUNK on GPUs. We use here ExaModels.jl [@shin2024accelerating] to model an optimization problem as it implements the NLPModel API and is compatible with GPU backends.\n\nusing CUDA, ExaModels, JSOSolvers, NLPModels, OptimizationProblems\nproblem = \"woods\"\nnscal = 100\nmodel = OptimizationProblems.PureJuMP.eval(Meta.parse(problem))(n = nscal)\nnlp_gpu = ExaModels.ExaModel(model; backend=CUDABackend(), prod=true)\nstats = trunk(nlp_gpu)","category":"section"},{"location":"#Home","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"JSOSolvers.jl is a collection of Julia optimization solvers for nonlinear, potentially nonconvex, continuous optimization problems that are unconstrained or bound-constrained:\n\nmin f(x)     s.t.  ℓ ≤ x ≤ u\n\nwhere fmathbbR^n rightarrow mathbbR is a continuously differentiable function, with  ell in left(mathbbR cup -infty right)^n, and  u in left(mathbbR cup +infty right)^n. The algorithms implemented here are iterative methods that aim to compute a stationary point of \\eqref{eq:nlp} using first and, if possible, second-order derivatives.\n\nThis package provides optimization solvers curated by the JuliaSmoothOptimizers organization. Solvers in JSOSolvers.jl take as input an AbstractNLPModel, JSO's general model API defined in NLPModels.jl, a flexible data type to evaluate objective and constraints, their derivatives, and to provide any information that a solver might request from a model.\n\nThe solvers in JSOSolvers.jl adopt a matrix-free approach, where standard optimization methods are implemented without forming derivative matrices explicitly. This strategy enables the solution of large-scale problems even when function and gradient evaluations are expensive. The motivation is to solve large-scale unconstrained and bound-constrained problems such as parameter estimation in inverse problems, design optimization in engineering, and regularized machine learning models, and use these solvers to solve subproblems of penalty algorithms.","category":"section"},{"location":"#Installation","page":"JSOSolvers.jl documentation","title":"Installation","text":"JSOSolvers is a registered package. To install this package, open the Julia REPL (i.e., execute the julia binary), type ] to enter package mode, and install JSOSolvers as follows\n\n`pkg> add JSOSolvers`","category":"section"},{"location":"#Bug-reports-and-discussions","page":"JSOSolvers.jl documentation","title":"Bug reports and discussions","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.\n\nIf you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"section"},{"location":"#Basic-usage","page":"JSOSolvers.jl documentation","title":"Basic usage","text":"All solvers here are JSO-Compliant, in the sense that they accept NLPModels and return GenericExecutionStats. This allows benchmark them easily.\n\nAll solvers can be called like the following:\n\nstats = solver_name(nlp; kwargs...)\n\nwhere nlp is an AbstractNLPModel or some specialization, such as an AbstractNLSModel, and the following keyword arguments are supported:\n\nx is the starting default (default: nlp.meta.x0);\natol is the absolute stopping tolerance (default: atol = √ϵ);\nrtol is the relative stopping tolerance (default: rtol = √ϵ);\nmax_eval is the maximum number of objective and constraints function evaluations (default: -1, which means no limit);\nmax_time is the maximum allowed elapsed time (default: 30.0);\nstats is a SolverTools.GenericExecutionStats with the output of the solver.\n\nSee the full list of Solvers.","category":"section"}]
}
