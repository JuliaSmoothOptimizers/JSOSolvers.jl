<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · JSOSolvers.jl</title><meta name="title" content="Reference · JSOSolvers.jl"/><meta property="og:title" content="Reference · JSOSolvers.jl"/><meta property="twitter:title" content="Reference · JSOSolvers.jl"/><meta name="description" content="Documentation for JSOSolvers.jl."/><meta property="og:description" content="Documentation for JSOSolvers.jl."/><meta property="twitter:description" content="Documentation for JSOSolvers.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="JSOSolvers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">JSOSolvers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">JSOSolvers.jl documentation</a></li><li><a class="tocitem" href="../solvers/">Solvers</a></li><li><a class="tocitem" href="../benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../floating-point-systems/">Multiple Floating-Point Systems Support</a></li><li><a class="tocitem" href="../performance-tips/">Performance Tips</a></li><li><a class="tocitem" href="../internal/">Internal functions</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/main/docs/src/reference.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><p>​</p><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><p>​</p><ul><li><a href="#Reference">Reference</a></li><li class="no-marker"><ul><li><a href="#Contents">Contents</a></li><li><a href="#Index">Index</a></li></ul></li></ul><p>​</p><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><p>​</p><ul><li><a href="#JSOSolvers.FOMOParameterSet"><code>JSOSolvers.FOMOParameterSet</code></a></li><li><a href="#JSOSolvers.FoSolver"><code>JSOSolvers.FoSolver</code></a></li><li><a href="#JSOSolvers.FomoSolver"><code>JSOSolvers.FomoSolver</code></a></li><li><a href="#JSOSolvers.LBFGSParameterSet"><code>JSOSolvers.LBFGSParameterSet</code></a></li><li><a href="#JSOSolvers.LBFGSSolver"><code>JSOSolvers.LBFGSSolver</code></a></li><li><a href="#JSOSolvers.R2Solver"><code>JSOSolvers.R2Solver</code></a></li><li><a href="#JSOSolvers.TRONLSParameterSet"><code>JSOSolvers.TRONLSParameterSet</code></a></li><li><a href="#JSOSolvers.TRONParameterSet"><code>JSOSolvers.TRONParameterSet</code></a></li><li><a href="#JSOSolvers.TRUNKLSParameterSet"><code>JSOSolvers.TRUNKLSParameterSet</code></a></li><li><a href="#JSOSolvers.TRUNKParameterSet"><code>JSOSolvers.TRUNKParameterSet</code></a></li><li><a href="#JSOSolvers.TronSolver"><code>JSOSolvers.TronSolver</code></a></li><li><a href="#JSOSolvers.TronSolverNLS"><code>JSOSolvers.TronSolverNLS</code></a></li><li><a href="#JSOSolvers.TrunkSolver"><code>JSOSolvers.TrunkSolver</code></a></li><li><a href="#JSOSolvers.TrunkSolverNLS"><code>JSOSolvers.TrunkSolverNLS</code></a></li><li><a href="#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.R2</code></a></li><li><a href="#JSOSolvers.TR-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.TR</code></a></li><li><a href="#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy!</code></a></li><li><a href="#JSOSolvers.cauchy_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy_ls!</code></a></li><li><a href="#JSOSolvers.find_beta-Union{Tuple{V}, Tuple{T}, Tuple{V, Vararg{T, 8}}} where {T, V}"><code>JSOSolvers.find_beta</code></a></li><li><a href="#JSOSolvers.fo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.fo</code></a></li><li><a href="#JSOSolvers.fomo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.fomo</code></a></li><li><a href="#JSOSolvers.init_alpha-Union{Tuple{T}, Tuple{T, r2_step}} where T"><code>JSOSolvers.init_alpha</code></a></li><li><a href="#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.lbfgs</code></a></li><li><a href="#JSOSolvers.normM!-NTuple{4, Any}"><code>JSOSolvers.normM!</code></a></li><li><a href="#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{TronSolverNLS{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_gauss_newton!</code></a></li><li><a href="#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, Real}} where T&lt;:Real"><code>JSOSolvers.projected_line_search!</code></a></li><li><a href="#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search_ls!</code></a></li><li><a href="#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_newton!</code></a></li><li><a href="#JSOSolvers.step_mult-Union{Tuple{T}, Tuple{T, T, r2_step}} where T"><code>JSOSolvers.step_mult</code></a></li><li><a href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a></li><li><a href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a></li><li><a href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V"><code>JSOSolvers.trunk</code></a></li><li><a href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V"><code>JSOSolvers.trunk</code></a></li></ul><p>​</p><article><details class="docstring" open="true"><summary id="JSOSolvers.FOMOParameterSet"><a class="docstring-binding" href="#JSOSolvers.FOMOParameterSet"><code>JSOSolvers.FOMOParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">FOMOParameterSet{T} &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>fomo</code> regroups the following parameters:</p><ul><li><code>η1</code>, <code>η2</code>: step acceptance parameters.</li><li><code>γ1</code>, <code>γ2</code>: regularization update parameters.</li><li><code>γ3</code> : momentum factor βmax update parameter in case of unsuccessful iteration.</li><li><code>αmax</code>: maximum step parameter for fomo algorithm.</li><li><code>β ∈ [0,1)</code>: target decay rate for the momentum.</li><li><code>θ1</code>: momentum contribution parameter for convergence condition (1).</li><li><code>θ2</code>: momentum contribution parameter for convergence condition (2). </li><li><code>M</code> : requires objective decrease over the <code>M</code> last iterates (nonmonotone context). <code>M=1</code> implies monotone behaviour. </li><li><code>step_backend</code>: step computation mode. Options are <code>r2_step()</code> for quadratic regulation step and <code>tr_step()</code> for first-order trust-region.</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">FOMOParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>η1::T = eps(T)^(1 // 4)</code></li><li><code>η2::T = T(95/100)</code></li><li><code>γ1::T = T(1/2)</code></li><li><code>γ2::T = T(2)</code></li><li><code>γ3::T = T(1/2)</code></li><li><code>αmax::T = 1/eps(T)</code></li><li><code>β = T(9/10) ∈ [0,1)</code></li><li><code>θ1 = T(1/10)</code></li><li><code>θ2 = eps(T)^(1/3)</code></li><li><code>M = 1</code></li><li>`step<em>backend = r2</em>step()</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L27-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.FoSolver"><a class="docstring-binding" href="#JSOSolvers.FoSolver"><code>JSOSolvers.FoSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">fo(nlp; kwargs...)
R2(nlp; kwargs...)
TR(nlp; kwargs...)</code></pre><p>A First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><code>R2</code> and <code>TR</code> runs <code>fo</code> with the dedicated <code>step_backend</code> keyword argument.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>η2 = T(95/100)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ1 = T(1/2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ2 = T(2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>αmax = 1/eps(T)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>M = 1</code> : algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L269-L334">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.FomoSolver"><a class="docstring-binding" href="#JSOSolvers.FomoSolver"><code>JSOSolvers.FomoSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">fomo(nlp; kwargs...)</code></pre><p>A First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p><strong>Algorithm description</strong></p><p>The step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) <em>. ∇f(xk) + βmax .</em> mk‖       (2) In the nonmonotone case, (1) rewrites (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk + (fm - fk)/μk ≥ θ1 * ‖∇f(xk)‖², with fm the largest objective value over the last M successful iterations, and fk = f(xk).</p><p><strong>Advanced usage</strong></p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FomoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>No momentum</strong>: if the user does not whish to use momentum (<code>β</code> = 0), it is recommended to use the memory-optimized <code>fo</code> method.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>, <code>η2 = T(95/100)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = T(2)</code>: regularization update parameters.</li><li><code>γ3 = T(1/2)</code> : momentum factor βmax update parameter in case of unsuccessful iteration.</li><li><code>αmax = 1/eps(T)</code>: maximum step parameter for fomo algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of objective evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(9/10) ∈ [0,1)</code>: target decay rate for the momentum.</li><li><code>θ1 = T(1/10)</code>: momentum contribution parameter for convergence condition (1).</li><li><code>θ2 = eps(T)^(1/3)</code>: momentum contribution parameter for convergence condition (2). </li><li><code>M = 1</code> : requires objective decrease over the <code>M</code> last iterates (nonmonotone context). <code>M=1</code> implies monotone behaviour. </li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: step computation mode. Options are <code>r2_step()</code> for quadratic regulation step and <code>tr_step()</code> for first-order trust-region.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user || stats.stats = :unknown</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><p><strong><code>fomo</code></strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fomo(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FomoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L105-L202">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.LBFGSParameterSet"><a class="docstring-binding" href="#JSOSolvers.LBFGSParameterSet"><code>JSOSolvers.LBFGSParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">LBFGSParameterSet{T} &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>lbfgs</code> regroups the following parameters:</p><ul><li><code>mem</code>: memory parameter of the <code>lbfgs</code> algorithm</li><li><code>τ₁</code>: slope factor in the Wolfe condition when performing the line search</li><li><code>bk_max</code>: maximum number of backtracks when performing the line search.</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">LBFGSParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>mem::Int = 5</code></li><li><code>τ₁::T = T(0.9999)</code></li><li><code>bk_max:: Int = 25</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/lbfgs.jl#L8-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.LBFGSSolver"><a class="docstring-binding" href="#JSOSolvers.LBFGSSolver"><code>JSOSolvers.LBFGSSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="language-julia hljs">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>bk_max:: Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/lbfgs.jl#L47-L99">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.R2Solver"><a class="docstring-binding" href="#JSOSolvers.R2Solver"><code>JSOSolvers.R2Solver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">`R2Solver` is deprecated, please check the documentation of `R2`.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L353-L355">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TRONLSParameterSet"><a class="docstring-binding" href="#JSOSolvers.TRONLSParameterSet"><code>JSOSolvers.TRONLSParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TRONLSParameterSet{T} &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>tron</code> regroups the following parameters:</p><ul><li><code>μ₀</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁</code>: algorithm parameter in (0, +∞).</li><li><code>σ</code>: algorithm parameter in (1, +∞).</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">TRONLSParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>μ₀::T = T(1 / 100)</code></li><li><code>μ₁::T = T(1)</code></li><li><code>σ::T = T(10)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L12-L30">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TRONParameterSet"><a class="docstring-binding" href="#JSOSolvers.TRONParameterSet"><code>JSOSolvers.TRONParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TRONParameterSet{T} &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>tron</code> regroups the following parameters:</p><ul><li><code>μ₀::T</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T</code>: algorithm parameter in (1, +∞).</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">TRONParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>μ₀::T = T(1 / 100)</code></li><li><code>μ₁::T = T(1)</code></li><li><code>σ::T = T(10)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L13-L31">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TRUNKLSParameterSet"><a class="docstring-binding" href="#JSOSolvers.TRUNKLSParameterSet"><code>JSOSolvers.TRUNKLSParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TRUNKLSParameterSet &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>tron</code> regroups the following parameters:</p><ul><li><code>bk_max</code>: algorithm parameter.</li><li><code>monotone</code>: algorithm parameter.</li><li><code>nm_itmax</code>: algorithm parameter.</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">TRUNKLSParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>bk_max::Int = 10</code></li><li><code>monotone::Bool = true</code></li><li><code>nm_itmax::Int = 25</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunkls.jl#L13-L31">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TRUNKParameterSet"><a class="docstring-binding" href="#JSOSolvers.TRUNKParameterSet"><code>JSOSolvers.TRUNKParameterSet</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TRUNKParameterSet &lt;: AbstractParameterSet</code></pre><p>This structure designed for <code>tron</code> regroups the following parameters:</p><ul><li><code>bk_max</code>: algorithm parameter.</li><li><code>monotone</code>: algorithm parameter.</li><li><code>nm_itmax</code>: algorithm parameter.</li></ul><p>An additional constructor is</p><pre><code class="language-julia hljs">TRUNKParameterSet(nlp: kwargs...)</code></pre><p>where the kwargs are the parameters above.</p><p>Default values are:</p><ul><li><code>bk_max::Int = 10</code></li><li><code>monotone::Bool = true</code></li><li><code>nm_itmax::Int = 25</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunk.jl#L10-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TronSolver"><a class="docstring-binding" href="#JSOSolvers.TronSolver"><code>JSOSolvers.TronSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="language-julia hljs">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1 / 100)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>μ₁::T = T(1)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>σ::T = T(10)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://jso.dev/SolverTools.jl/stable/reference/#SolverTools.TRONTrustRegion"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="language-julia hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L52-L118">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TronSolverNLS"><a class="docstring-binding" href="#JSOSolvers.TronSolverNLS"><code>JSOSolvers.TronSolverNLS</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="language-julia hljs">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls, subsolver::Symbol=:lsmr; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver::Symbol = :lsmr</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1 / 100)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>μ₁::T = T(1)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>σ::T = T(10)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://jso.dev/SolverTools.jl/stable/reference/#SolverTools.TRONTrustRegion"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="language-julia hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L51-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TrunkSolver"><a class="docstring-binding" href="#JSOSolvers.TrunkSolver"><code>JSOSolvers.TrunkSolver</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TrunkSolver(nlp, subsolver::Symbol = :cg)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>monotone::Bool = true</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li><li><code>M</code>: linear operator that models a Hermitian positive-definite matrix of size <code>n</code>; passed to Krylov subsolvers. </li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-julia hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunk.jl#L49-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TrunkSolverNLS"><a class="docstring-binding" href="#JSOSolvers.TrunkSolverNLS"><code>JSOSolvers.TrunkSolverNLS</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="language-julia hljs">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TrunkSolverNLS(nls, subsolver::Symbol = :lsmr)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>monotone::Bool = true</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available Krylov solvers.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-julia hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunkls.jl#L52-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.R2</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fo(nlp; kwargs...)
R2(nlp; kwargs...)
TR(nlp; kwargs...)</code></pre><p>A First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><code>R2</code> and <code>TR</code> runs <code>fo</code> with the dedicated <code>step_backend</code> keyword argument.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>η2 = T(95/100)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ1 = T(1/2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ2 = T(2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>αmax = 1/eps(T)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>M = 1</code> : algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L269-L334">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.TR-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.TR-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.TR</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fo(nlp; kwargs...)
R2(nlp; kwargs...)
TR(nlp; kwargs...)</code></pre><p>A First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><code>R2</code> and <code>TR</code> runs <code>fo</code> with the dedicated <code>step_backend</code> keyword argument.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>η2 = T(95/100)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ1 = T(1/2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ2 = T(2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>αmax = 1/eps(T)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>M = 1</code> : algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L269-L334">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy!</code></a> — <span class="docstring-category">Method</span></summary><section><div><p><code>α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)</code></p><p>Computes a Cauchy step <code>s = P(x - α g) - x</code> for</p><pre><code class="language-julia hljs">min  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,</code></pre><p>with the sufficient decrease condition</p><pre><code class="language-julia hljs">q(s) ≦ μ₀sᵀg.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L501-L511">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.cauchy_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.cauchy_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy_ls!</code></a> — <span class="docstring-category">Method</span></summary><section><div><p><code>α, s = cauchy_ls!(x, A, Fx, g, Δ, ℓ, u, s, As; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)</code></p><p>Computes a Cauchy step <code>s = P(x - α g) - x</code> for</p><pre><code class="language-julia hljs">min  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,</code></pre><p>with the sufficient decrease condition</p><pre><code class="language-julia hljs">q(s) ≦ μ₀gᵀs,</code></pre><p>where g = AᵀFx.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L521-L533">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.find_beta-Union{Tuple{V}, Tuple{T}, Tuple{V, Vararg{T, 8}}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.find_beta-Union{Tuple{V}, Tuple{T}, Tuple{V, Vararg{T, 8}}} where {T, V}"><code>JSOSolvers.find_beta</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">find_beta(m, mdot∇f, norm_∇f, μk, fk, max_obj_mem, β, θ1, θ2)</code></pre><p>Compute βmax which saturates the contribution of the momentum term to the gradient. <code>βmax</code> is computed such that the two gradient-related conditions (first one is relaxed in the nonmonotone case) are ensured: </p><ol><li>(1-βmax) * ‖∇f(xk)‖² + βmax * ∇f(xk)ᵀm + (max<em>obj</em>mem - fk)/μk ≥ θ1 * ‖∇f(xk)‖²</li><li>‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) * ∇f(xk) .+ βmax .* m‖</li></ol><p>with <code>m</code> the momentum term and <code>mdot∇f = ∇f(xk)ᵀm</code>, <code>fk</code> the model at s=0, <code>max_obj_mem</code> the largest objective value over the last M successful iterations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L614-L622">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.fo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.fo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.fo</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fo(nlp; kwargs...)
R2(nlp; kwargs...)
TR(nlp; kwargs...)</code></pre><p>A First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><code>R2</code> and <code>TR</code> runs <code>fo</code> with the dedicated <code>step_backend</code> keyword argument.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>η2 = T(95/100)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ1 = T(1/2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>γ2 = T(2)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>αmax = 1/eps(T)</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>M = 1</code> : algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: algorithm parameter, see <a href="#JSOSolvers.FOMOParameterSet"><code>FOMOParameterSet</code></a>.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L269-L334">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.fomo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.fomo-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.fomo</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fomo(nlp; kwargs...)</code></pre><p>A First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p><strong>Algorithm description</strong></p><p>The step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) <em>. ∇f(xk) + βmax .</em> mk‖       (2) In the nonmonotone case, (1) rewrites (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk + (fm - fk)/μk ≥ θ1 * ‖∇f(xk)‖², with fm the largest objective value over the last M successful iterations, and fk = f(xk).</p><p><strong>Advanced usage</strong></p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = FomoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>No momentum</strong>: if the user does not whish to use momentum (<code>β</code> = 0), it is recommended to use the memory-optimized <code>fo</code> method.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1 // 4)</code>, <code>η2 = T(95/100)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = T(2)</code>: regularization update parameters.</li><li><code>γ3 = T(1/2)</code> : momentum factor βmax update parameter in case of unsuccessful iteration.</li><li><code>αmax = 1/eps(T)</code>: maximum step parameter for fomo algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of objective evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(9/10) ∈ [0,1)</code>: target decay rate for the momentum.</li><li><code>θ1 = T(1/10)</code>: momentum contribution parameter for convergence condition (1).</li><li><code>θ2 = eps(T)^(1/3)</code>: momentum contribution parameter for convergence condition (2). </li><li><code>M = 1</code> : requires objective decrease over the <code>M</code> last iterates (nonmonotone context). <code>M=1</code> implies monotone behaviour. </li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: step computation mode. Options are <code>r2_step()</code> for quadratic regulation step and <code>tr_step()</code> for first-order trust-region.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user || stats.stats = :unknown</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><p><strong><code>fomo</code></strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fomo(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FomoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L105-L202">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.init_alpha-Union{Tuple{T}, Tuple{T, r2_step}} where T"><a class="docstring-binding" href="#JSOSolvers.init_alpha-Union{Tuple{T}, Tuple{T, r2_step}} where T"><code>JSOSolvers.init_alpha</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">init_alpha(norm_∇fk::T, ::r2_step)
init_alpha(norm_∇fk::T, ::tr_step)</code></pre><p>Initialize <code>α</code> step size parameter. Ensure first step is the same for quadratic regularization and trust region methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L641-L647">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.lbfgs</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="language-julia hljs">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>bk_max:: Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.LBFGSParameterSet"><code>LBFGSParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/lbfgs.jl#L47-L99">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.normM!-NTuple{4, Any}"><a class="docstring-binding" href="#JSOSolvers.normM!-NTuple{4, Any}"><code>JSOSolvers.normM!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">normM!(n, x, M, z)</code></pre><p>Weighted norm of <code>x</code> with respect to <code>M</code>, i.e., <code>z = sqrt(x&#39; * M * x)</code>. Uses <code>z</code> as workspace.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/JSOSolvers.jl#L30-L34">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{TronSolverNLS{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{TronSolverNLS{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_gauss_newton!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">projected_gauss_newton!(solver, x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u; max_cgiter = 50, max_time = Inf, subsolver_verbose = 0)</code></pre><p>Compute an approximate solution <code>d</code> for</p><pre><code class="language-julia hljs">min q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ</code></pre><p>starting from <code>s</code>.  The steps are computed using the conjugate gradient method projected on the active bounds.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L608-L618">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, Real}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}, Real}} where T&lt;:Real"><code>JSOSolvers.projected_line_search!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">s = projected_line_search!(x, H, g, d, ℓ, u, Hs, μ₀)</code></pre><p>Performs a projected line search, searching for a step size <code>t</code> such that</p><pre><code class="language-julia hljs">0.5sᵀHs + sᵀg ≦ μ₀sᵀg,</code></pre><p>where <code>s = P(x + t * d) - x</code>, while remaining on the same face as <code>x + d</code>. Backtracking is performed from t = 1.0. <code>x</code> is updated in place.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L450-L459">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search_ls!</code></a> — <span class="docstring-category">Method</span></summary><section><div><p><code>s = projected_line_search_ls!(x, A, g, d, ℓ, u, As, s; μ₀ = 1e-2)</code></p><p>Performs a projected line search, searching for a step size <code>t</code> such that</p><pre><code class="language-julia hljs">½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs</code></pre><p>where <code>s = P(x + t * d) - x</code>, while remaining on the same face as <code>x + d</code>. Backtracking is performed from t = 1.0. <code>x</code> is updated in place.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L469-L478">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><a class="docstring-binding" href="#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Sub, Op, Aop} where {V&lt;:AbstractVector{T}, Sub&lt;:Krylov.KrylovWorkspace{T, T, V}, Op&lt;:LinearOperators.AbstractLinearOperator{T}, Aop&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_newton!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_time = Inf, max_cgiter = 50, subsolver_verbose = 0)</code></pre><p>Compute an approximate solution <code>d</code> for</p><p>min q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ</p><p>starting from <code>s</code>.  The steps are computed using the conjugate gradient method projected on the active bounds.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L585-L595">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.step_mult-Union{Tuple{T}, Tuple{T, T, r2_step}} where T"><a class="docstring-binding" href="#JSOSolvers.step_mult-Union{Tuple{T}, Tuple{T, T, r2_step}} where T"><code>JSOSolvers.step_mult</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">step_mult(α::T, norm_∇fk::T, ::r2_step)
step_mult(α::T, norm_∇fk::T, ::tr_step)</code></pre><p>Compute step size multiplier: <code>α</code> for quadratic regularization(<code>::r2</code> and <code>::R2og</code>) and <code>α/norm_∇fk</code> for trust region (<code>::tr</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/fomo.jl#L656-L661">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="language-julia hljs">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls, subsolver::Symbol=:lsmr; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver::Symbol = :lsmr</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1 / 100)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>μ₁::T = T(1)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>σ::T = T(10)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONLSParameterSet"><code>TRONLSParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://jso.dev/SolverTools.jl/stable/reference/#SolverTools.TRONTrustRegion"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="language-julia hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tronls.jl#L51-L119">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}"><a class="docstring-binding" href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="language-julia hljs">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1 / 100)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>μ₁::T = T(1)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>σ::T = T(10)</code>: algorithm parameter, see <a href="#JSOSolvers.TRONParameterSet"><code>TRONParameterSet</code></a>.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖x - Proj(x - ∇f(xᵏ))‖ ≤ atol + rtol * ‖∇f(x⁰)‖. Proj denotes here the projection over the bounds.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://jso.dev/SolverTools.jl/stable/reference/#SolverTools.TRONTrustRegion"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="language-julia hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/tron.jl#L52-L118">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V"><a class="docstring-binding" href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="language-julia hljs">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TrunkSolverNLS(nls, subsolver::Symbol = :lsmr)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>monotone::Bool = true</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKLSParameterSet"><code>TRUNKLSParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available Krylov solvers.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-julia hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunkls.jl#L52-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V"><a class="docstring-binding" href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-julia hljs">solver = TrunkSolver(nlp, subsolver::Symbol = :cg)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>monotone::Bool = true</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter, see <a href="#JSOSolvers.TRUNKParameterSet"><code>TRUNKParameterSet</code></a>.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li><li><code>M</code>: linear operator that models a Hermitian positive-definite matrix of size <code>n</code>; passed to Krylov subsolvers. </li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of the residual, for instance, the norm of the gradient for unconstrained problems;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-julia hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/64981f442900f96ea08ff22b31bbca993647a816/src/trunk.jl#L49-L115">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../internal/">« Internal functions</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Sunday 23 November 2025 22:29">Sunday 23 November 2025</span>. Using Julia version 1.12.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
