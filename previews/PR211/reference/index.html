<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · JSOSolvers.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="JSOSolvers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">JSOSolvers.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../solvers/">Solvers</a></li><li><a class="tocitem" href="../internal/">Internal</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/master/docs/src/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><p>​</p><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><p>​</p><ul><li><a href="#Reference">Reference</a></li><ul><li><a href="#Contents">Contents</a></li><li><a href="#Index">Index</a></li></ul></ul><p>​</p><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><p>​</p><ul><li><a href="#JSOSolvers.LBFGSSolver"><code>JSOSolvers.LBFGSSolver</code></a></li><li><a href="#JSOSolvers.R2Solver"><code>JSOSolvers.R2Solver</code></a></li><li><a href="#JSOSolvers.TronSolver"><code>JSOSolvers.TronSolver</code></a></li><li><a href="#JSOSolvers.TronSolverNLS"><code>JSOSolvers.TronSolverNLS</code></a></li><li><a href="#JSOSolvers.TrunkSolver"><code>JSOSolvers.TrunkSolver</code></a></li><li><a href="#JSOSolvers.TrunkSolverNLS"><code>JSOSolvers.TrunkSolverNLS</code></a></li><li><a href="#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.R2</code></a></li><li><a href="#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy!</code></a></li><li><a href="#JSOSolvers.cauchy_ls-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.cauchy_ls</code></a></li><li><a href="#JSOSolvers.compute_As_slope_qs!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.compute_As_slope_qs!</code></a></li><li><a href="#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel}, Tuple{V}} where V"><code>JSOSolvers.lbfgs</code></a></li><li><a href="#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.projected_gauss_newton!</code></a></li><li><a href="#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search!</code></a></li><li><a href="#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search_ls!</code></a></li><li><a href="#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Op} where {V&lt;:AbstractVector{T}, Op&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_newton!</code></a></li><li><a href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a></li><li><a href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a></li><li><a href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V"><code>JSOSolvers.trunk</code></a></li><li><a href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V"><code>JSOSolvers.trunk</code></a></li></ul><p>​</p><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.LBFGSSolver" href="#JSOSolvers.LBFGSSolver"><code>JSOSolvers.LBFGSSolver</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="language-none">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: memory parameter of the <code>lbfgs</code> algorithm.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: slope factor in the Wolfe condition when performing the line search.</li><li><code>bk_max:: Int = 25</code>: maximum number of backtracks when performing the line search.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/lbfgs.jl#L3-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.R2Solver" href="#JSOSolvers.R2Solver"><code>JSOSolvers.R2Solver</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">R2(nlp; kwargs...)</code></pre><p>A first-order quadratic regularization method for unconstrained optimization.</p><p>For advanced usage, first define a <code>R2Solver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = R2Solver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1/4)</code>, <code>η2 = T(0.95)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = 1/γ1</code>: regularization update parameters.</li><li><code>σmin = eps(T)</code>: step parameter for R2 algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(0) ∈ [0,1]</code> is the constant in the momentum term. If <code>β == 0</code>, R2 does not use momentum.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = R2Solver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/R2.jl#L3-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.TronSolver" href="#JSOSolvers.TronSolver"><code>JSOSolvers.TronSolver</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="language-none">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tron.jl#L8-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.TronSolverNLS" href="#JSOSolvers.TronSolverNLS"><code>JSOSolvers.TronSolverNLS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver::Symbol = :lsmr</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L24-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.TrunkSolver" href="#JSOSolvers.TrunkSolver"><code>JSOSolvers.TrunkSolver</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolver(nlp, subsolver_type::Type{&lt;:KrylovSolver} = CgSolver)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/trunk.jl#L5-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.TrunkSolverNLS" href="#JSOSolvers.TrunkSolverNLS"><code>JSOSolvers.TrunkSolverNLS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolverNLS(nls, subsolver_type::Type{&lt;:KrylovSolver} = LsmrSolver)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available <code>KrylovSolver</code>.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/trunkls.jl#L8-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}" href="#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}"><code>JSOSolvers.R2</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">R2(nlp; kwargs...)</code></pre><p>A first-order quadratic regularization method for unconstrained optimization.</p><p>For advanced usage, first define a <code>R2Solver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = R2Solver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1/4)</code>, <code>η2 = T(0.95)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = 1/γ1</code>: regularization update parameters.</li><li><code>σmin = eps(T)</code>: step parameter for R2 algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(0) ∈ [0,1]</code> is the constant in the momentum term. If <code>β == 0</code>, R2 does not use momentum.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = R2Solver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/R2.jl#L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real" href="#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.cauchy!</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)</code></p><p>Computes a Cauchy step <code>s = P(x - α g) - x</code> for</p><pre><code class="language-none">min  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,</code></pre><p>with the sufficient decrease condition</p><pre><code class="language-none">q(s) ≦ μ₀sᵀg.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tron.jl#L430-L440">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.cauchy_ls-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real" href="#JSOSolvers.cauchy_ls-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.cauchy_ls</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>α, s = cauchy_ls(x, A, Fx, g, Δ, ℓ, u; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)</code></p><p>Computes a Cauchy step <code>s = P(x - α g) - x</code> for</p><pre><code class="language-none">min  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,</code></pre><p>with the sufficient decrease condition</p><pre><code class="language-none">q(s) ≦ μ₀gᵀs,</code></pre><p>where g = AᵀFx.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L423-L435">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.compute_As_slope_qs!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real" href="#JSOSolvers.compute_As_slope_qs!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.compute_As_slope_qs!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">slope, qs = compute_As_slope_qs!(As, A, s, Fx)</code></pre><p>Compute <code>slope = dot(As, Fx)</code> and <code>qs = dot(As, As) / 2 + slope</code>. Use <code>As</code> to store <code>A * s</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel}, Tuple{V}} where V" href="#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel}, Tuple{V}} where V"><code>JSOSolvers.lbfgs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="language-none">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: memory parameter of the <code>lbfgs</code> algorithm.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: slope factor in the Wolfe condition when performing the line search.</li><li><code>bk_max:: Int = 25</code>: maximum number of backtracks when performing the line search.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/lbfgs.jl#L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real" href="#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, AbstractVector{T}, AbstractVector{T}, AbstractVector{T}}} where T&lt;:Real"><code>JSOSolvers.projected_gauss_newton!</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>projected_gauss_newton!(x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u)</code></p><p>Compute an approximate solution <code>d</code> for</p><pre><code class="language-none">min q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ</code></pre><p>starting from <code>s</code>.  The steps are computed using the conjugate gradient method projected on the active bounds.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L507-L516">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real" href="#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search!</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>s = projected_line_search!(x, H, g, d, ℓ, u, Hs; μ₀ = 1e-2)</code></p><p>Performs a projected line search, searching for a step size <code>t</code> such that</p><pre><code class="language-none">0.5sᵀHs + sᵀg ≦ μ₀sᵀg,</code></pre><p>where <code>s = P(x + t * d) - x</code>, while remaining on the same face as <code>x + d</code>. Backtracking is performed from t = 1.0. <code>x</code> is updated in place.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tron.jl#L380-L389">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real" href="#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_line_search_ls!</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>s = projected_line_search_ls!(x, A, g, d, ℓ, u; μ₀ = 1e-2)</code></p><p>Performs a projected line search, searching for a step size <code>t</code> such that</p><pre><code class="language-none">½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs</code></pre><p>where <code>s = P(x + t * d) - x</code>, while remaining on the same face as <code>x + d</code>. Backtracking is performed from t = 1.0. <code>x</code> is updated in place.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L373-L382">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Op} where {V&lt;:AbstractVector{T}, Op&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real" href="#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Op} where {V&lt;:AbstractVector{T}, Op&lt;:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T&lt;:Real"><code>JSOSolvers.projected_newton!</code></a> — <span class="docstring-category">Method</span></header><section><div><p><code>projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_cgiter = 50)</code></p><p>Compute an approximate solution <code>d</code> for</p><p>min q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ</p><p>starting from <code>s</code>.  The steps are computed using the conjugate gradient method projected on the active bounds.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tron.jl#L514-L523">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}" href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver::Symbol = :lsmr</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tronls.jl#L145">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}" href="#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="language-none">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="language-none">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/tron.jl#L172">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V" href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="language-none">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolverNLS(nls, subsolver_type::Type{&lt;:KrylovSolver} = LsmrSolver)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available <code>KrylovSolver</code>.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/trunkls.jl#L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V" href="#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="language-none">solver = TrunkSolver(nlp, subsolver_type::Type{&lt;:KrylovSolver} = CgSolver)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="language-none">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/1d8938a8a7aacff271de641440bb774f3efe6568/src/trunk.jl#L134">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../internal/">« Internal</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 24 May 2023 19:47">Wednesday 24 May 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
