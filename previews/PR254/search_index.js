var documenterSearchIndex = {"docs":
[{"location":"solvers/#Solvers","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Solver list","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"lbfgs\ntron\ntrunk\nR2","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Problem type Solvers\nUnconstrained NLP lbfgs, tron, trunk, R2\nUnconstrained NLS trunk, tron\nBound-constrained NLP tron\nBound-constrained NLS tron","category":"page"},{"location":"solvers/#Solver-list","page":"Solvers","title":"Solver list","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"lbfgs\ntron\ntrunk\nR2","category":"page"},{"location":"solvers/#JSOSolvers.lbfgs","page":"Solvers","title":"JSOSolvers.lbfgs","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: memory parameter of the lbfgs algorithm.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): slope factor in the Wolfe condition when performing the line search.\nbk_max:: Int = 25: maximum number of backtracks when performing the line search.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.tron","page":"Solvers","title":"JSOSolvers.tron","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\ntron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver_type::Symbol = LsmrSolver: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.trunk","page":"Solvers","title":"JSOSolvers.trunk","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver_type::Type{<:KrylovSolver} = CgSolver)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\ntrunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available KrylovSolver.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"function"},{"location":"solvers/#JSOSolvers.R2","page":"Solvers","title":"JSOSolvers.R2","text":"R2(nlp; kwargs...)\n\nA first-order quadratic regularization method for unconstrained optimization.\n\nFor advanced usage, first define a R2Solver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2Solver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1/4), η2 = T(0.95): step acceptance parameters.\nγ1 = T(1/2), γ2 = 1/γ1: regularization update parameters.\nσmin = eps(T): step parameter for R2 algorithm.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(0) ∈ [0,1] is the constant in the momentum term. If β == 0, R2 does not use momentum.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = R2Solver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"function"},{"location":"internal/#Internal-functions","page":"Internal functions","title":"Internal functions","text":"","category":"section"},{"location":"internal/","page":"Internal functions","title":"Internal functions","text":"JSOSolvers.projected_newton!\nJSOSolvers.projected_line_search!\nJSOSolvers.cauchy!\nJSOSolvers.projected_gauss_newton!\nJSOSolvers.projected_line_search_ls!\nJSOSolvers.cauchy_ls!","category":"page"},{"location":"internal/#JSOSolvers.projected_newton!","page":"Internal functions","title":"JSOSolvers.projected_newton!","text":"projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_line_search!","page":"Internal functions","title":"JSOSolvers.projected_line_search!","text":"s = projected_line_search!(x, H, g, d, ℓ, u, Hs; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n0.5sᵀHs + sᵀg ≦ μ₀sᵀg,\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.cauchy!","page":"Internal functions","title":"JSOSolvers.cauchy!","text":"α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀sᵀg.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_gauss_newton!","page":"Internal functions","title":"JSOSolvers.projected_gauss_newton!","text":"projected_gauss_newton!(solver, x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u; max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.projected_line_search_ls!","page":"Internal functions","title":"JSOSolvers.projected_line_search_ls!","text":"s = projected_line_search_ls!(x, A, g, d, ℓ, u, As, s; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"function"},{"location":"internal/#JSOSolvers.cauchy_ls!","page":"Internal functions","title":"JSOSolvers.cauchy_ls!","text":"α, s = cauchy_ls!(x, A, Fx, g, Δ, ℓ, u, s, As; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀gᵀs,\n\nwhere g = AᵀFx.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [JSOSolvers]","category":"page"},{"location":"reference/#JSOSolvers.LBFGSSolver","page":"Reference","title":"JSOSolvers.LBFGSSolver","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: memory parameter of the lbfgs algorithm.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): slope factor in the Wolfe condition when performing the line search.\nbk_max:: Int = 25: maximum number of backtracks when performing the line search.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.LbfgsParameterSet","page":"Reference","title":"JSOSolvers.LbfgsParameterSet","text":"LbfgsParameterSet{T} <: AbstractParameterSet\n\nThis structure designed for lbfgs regroup the following parameters:\n\nmem::Parameter{Int, IntegerRange{Int}}: memory parameter of the lbfgs algorithm (default: 5)\nτ₁::Parameter{T, RealInterval{T}}: slope factor in the Wolfe condition when performing the line search.\nbk_max::Parameter{Int, IntegerRange{Int}}: maximum number of backtracks when performing the line search.\n\nDefault values are:\n\nmem::Int = 5\nτ₁::T = T(0.9999)\nbk_max:: Int = 25\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.R2Solver","page":"Reference","title":"JSOSolvers.R2Solver","text":"R2(nlp; kwargs...)\n\nA first-order quadratic regularization method for unconstrained optimization.\n\nFor advanced usage, first define a R2Solver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2Solver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1/4), η2 = T(0.95): step acceptance parameters.\nγ1 = T(1/2), γ2 = 1/γ1: regularization update parameters.\nσmin = eps(T): step parameter for R2 algorithm.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(0) ∈ [0,1] is the constant in the momentum term. If β == 0, R2 does not use momentum.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = R2Solver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TronSolver","page":"Reference","title":"JSOSolvers.TronSolver","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TronSolverNLS","page":"Reference","title":"JSOSolvers.TronSolverNLS","text":"tron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver_type::Symbol = LsmrSolver: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TrunkSolver","page":"Reference","title":"JSOSolvers.TrunkSolver","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver_type::Type{<:KrylovSolver} = CgSolver)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.TrunkSolverNLS","page":"Reference","title":"JSOSolvers.TrunkSolverNLS","text":"trunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available KrylovSolver.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"type"},{"location":"reference/#JSOSolvers.R2-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.R2","text":"R2(nlp; kwargs...)\n\nA first-order quadratic regularization method for unconstrained optimization.\n\nFor advanced usage, first define a R2Solver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = R2Solver(nlp)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} is the model to solve, see NLPModels.jl.\n\nKeyword arguments\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nη1 = eps(T)^(1/4), η2 = T(0.95): step acceptance parameters.\nγ1 = T(1/2), γ2 = 1/γ1: regularization update parameters.\nσmin = eps(T): step parameter for R2 algorithm.\nmax_eval::Int = -1: maximum number of evaluation of the objective function.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nβ = T(0) ∈ [0,1] is the constant in the momentum term. If β == 0, R2 does not use momentum.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = R2(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = R2Solver(nlp);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.cauchy!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.cauchy!","text":"α, s = cauchy!(x, H, g, Δ, ℓ, u, s, Hs; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ¹/₂sᵀHs + gᵀs     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀sᵀg.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.cauchy_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, AbstractVector{T}, Real, Real, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.cauchy_ls!","text":"α, s = cauchy_ls!(x, A, Fx, g, Δ, ℓ, u, s, As; μ₀ = 1e-2, μ₁ = 1.0, σ=10.0)\n\nComputes a Cauchy step s = P(x - α g) - x for\n\nmin  q(s) = ½‖As + Fx‖² - ½‖Fx‖²     s.t.    ‖s‖ ≦ μ₁Δ,  ℓ ≦ x + s ≦ u,\n\nwith the sufficient decrease condition\n\nq(s) ≦ μ₀gᵀs,\n\nwhere g = AᵀFx.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.lbfgs-Union{Tuple{NLPModels.AbstractNLPModel{T, V}}, Tuple{V}, Tuple{T}} where {T, V}","page":"Reference","title":"JSOSolvers.lbfgs","text":"lbfgs(nlp; kwargs...)\n\nAn implementation of a limited memory BFGS line-search method for unconstrained minimization.\n\nFor advanced usage, first define a LBFGSSolver to preallocate the memory used in the algorithm, and then call solve!.\n\nsolver = LBFGSSolver(nlp; mem::Int = 5)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nmem::Int = 5: memory parameter of the lbfgs algorithm.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nτ₁::T = T(0.9999): slope factor in the Wolfe condition when performing the line search.\nbk_max:: Int = 25: maximum number of backtracks when performing the line search.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nverbose_subsolver::Int = 0: if > 0, display iteration information every verbose_subsolver iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nstats = lbfgs(nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3));\nsolver = LBFGSSolver(nlp; mem = 5);\nstats = solve!(solver, nlp)\n\n# output\n\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_gauss_newton!-Union{Tuple{T}, Tuple{TronSolverNLS{T, V, Sub, Op, Aop} where {V<:AbstractVector{T}, Sub<:Krylov.KrylovSolver{T, T, V}, Op<:LinearOperators.AbstractLinearOperator{T}, Aop<:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_gauss_newton!","text":"projected_gauss_newton!(solver, x, A, Fx, Δ, gctol, s, max_cgiter, ℓ, u; max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ½‖Ad + Fx‖² - ½‖Fx‖²     s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_line_search!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_line_search!","text":"s = projected_line_search!(x, H, g, d, ℓ, u, Hs; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n0.5sᵀHs + sᵀg ≦ μ₀sᵀg,\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_line_search_ls!-Union{Tuple{T}, Tuple{AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, Vararg{AbstractVector{T}, 6}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_line_search_ls!","text":"s = projected_line_search_ls!(x, A, g, d, ℓ, u, As, s; μ₀ = 1e-2)\n\nPerforms a projected line search, searching for a step size t such that\n\n½‖As + Fx‖² ≤ ½‖Fx‖² + μ₀FxᵀAs\n\nwhere s = P(x + t * d) - x, while remaining on the same face as x + d. Backtracking is performed from t = 1.0. x is updated in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.projected_newton!-Union{Tuple{T}, Tuple{TronSolver{T, V, Op, Aop} where {V<:AbstractVector{T}, Op<:LinearOperators.AbstractLinearOperator{T}, Aop<:LinearOperators.AbstractLinearOperator{T}}, AbstractVector{T}, Union{LinearOperators.AbstractLinearOperator, AbstractMatrix}, AbstractVector{T}, T, T, Vararg{AbstractVector{T}, 4}}} where T<:Real","page":"Reference","title":"JSOSolvers.projected_newton!","text":"projected_newton!(solver, x, H, g, Δ, cgtol, ℓ, u, s, Hs; max_cgiter = 50, subsolver_verbose = 0)\n\nCompute an approximate solution d for\n\nmin q(d) = ¹/₂dᵀHs + dᵀg    s.t.    ℓ ≦ x + d ≦ u,  ‖d‖ ≦ Δ\n\nstarting from s.  The steps are computed using the conjugate gradient method projected on the active bounds.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel{T, V}}} where {T, V}","page":"Reference","title":"JSOSolvers.tron","text":"tron(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:\n\nmin ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolverNLS to preallocate the memory used in the algorithm, and then call solve!:     solver = TronSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver; kwargs...)     solve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nsubsolver_type::Symbol = LsmrSolver: Krylov.jl method used as subproblem solver, see JSOSolvers.tronls_allowed_subsolvers for a list.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem iteration limit.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolverNLS are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nstats = tron(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))\nsolver = TronSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.tron-Union{Tuple{V}, Tuple{T}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel{T, V}}} where {T, V}","page":"Reference","title":"JSOSolvers.tron","text":"tron(nlp; kwargs...)\n\nA pure Julia implementation of a trust-region solver for bound-constrained optimization:\n\n    min f(x)    s.t.    ℓ ≦ x ≦ u\n\nFor advanced usage, first define a TronSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TronSolver(nlp; kwargs...)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\nμ₀::T = T(1e-2): algorithm parameter in (0, 0.5).\nμ₁::T = one(T): algorithm parameter in (0, +∞).\nσ::T = T(10): algorithm parameter in (1, +∞).\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nmax_cgiter::Int = 50: subproblem's iteration limit.\nuse_only_objgrad::Bool = false: If true, the algorithm uses only the function objgrad instead of obj and grad.\ncgtol::T = T(0.1): subproblem tolerance.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nThe keyword arguments of TronSolver are passed to the TRONTrustRegion constructor.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nTRON is described in\n\nChih-Jen Lin and Jorge J. Moré, *Newton's Method for Large Bound-Constrained\nOptimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.\nDOI: 10.1137/S1052623498345075\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nstats = tron(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x), ones(3), zeros(3), 2 * ones(3));\nsolver = TronSolver(nlp);\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:GaussNewton}, NLPModels.AbstractNLSModel}} where V","page":"Reference","title":"JSOSolvers.trunk","text":"trunk(nls; kwargs...)\n\nA pure Julia implementation of a trust-region solver for nonlinear least-squares problems:\n\nmin ½‖F(x)‖²\n\nFor advanced usage, first define a TrunkSolverNLS to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolverNLS(nls, subsolver_type::Type{<:KrylovSolver} = LsmrSolver)\nsolve!(solver, nls; kwargs...)\n\nArguments\n\nnls::AbstractNLSModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nFatol::T = √eps(T): absolute tolerance on the residual.\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nSee JSOSolvers.trunkls_allowed_subsolvers for a list of available KrylovSolver.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats = trunk(nls)\n\nusing JSOSolvers, ADNLPModels\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nsolver = TrunkSolverNLS(nls)\nstats = solve!(solver, nls)\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.trunk-Union{Tuple{V}, Tuple{Val{:Newton}, NLPModels.AbstractNLPModel}} where V","page":"Reference","title":"JSOSolvers.trunk","text":"trunk(nlp; kwargs...)\n\nA trust-region solver for unconstrained optimization using exact second derivatives.\n\nFor advanced usage, first define a TrunkSolver to preallocate the memory used in the algorithm, and then call solve!:\n\nsolver = TrunkSolver(nlp, subsolver_type::Type{<:KrylovSolver} = CgSolver)\nsolve!(solver, nlp; kwargs...)\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nsubsolver_logger::AbstractLogger = NullLogger(): subproblem's logger.\nx::V = nlp.meta.x0: the initial guess.\natol::T = √eps(T): absolute tolerance.\nrtol::T = √eps(T): relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.\nmax_eval::Int = -1: maximum number of objective function evaluations.\nmax_time::Float64 = 30.0: maximum time limit in seconds.\nmax_iter::Int = typemax(Int): maximum number of iterations.\nbk_max::Int = 10: algorithm parameter.\nmonotone::Bool = true: algorithm parameter.\nnm_itmax::Int = 25: algorithm parameter.\nverbose::Int = 0: if > 0, display iteration information every verbose iteration.\nsubsolver_verbose::Int = 0: if > 0, display iteration information every subsolver_verbose iteration of the subsolver.\n\nOutput\n\nThe returned value is a GenericExecutionStats, see SolverCore.jl.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver. Notably, you can access, and modify, the following:\n\nsolver.x: current iterate;\nsolver.gx: current gradient;\nstats: structure holding the output of the algorithm (GenericExecutionStats), which contains, among other things:\nstats.dual_feas: norm of current gradient;\nstats.iter: current iteration counter;\nstats.objective: current objective function value;\nstats.status: current status of the algorithm. Should be :unknown unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use :user to properly indicate the intention.\nstats.elapsed_time: elapsed time in seconds.\n\nReferences\n\nThis implementation follows the description given in\n\nA. R. Conn, N. I. M. Gould, and Ph. L. Toint,\nTrust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.\nSIAM, Philadelphia, USA, 2000.\nDOI: 10.1137/1.9780898719857\n\nThe main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.\n\nExamples\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nstats = trunk(nlp)\n\nusing JSOSolvers, ADNLPModels\nnlp = ADNLPModel(x -> sum(x.^2), ones(3))\nsolver = TrunkSolver(nlp)\nstats = solve!(solver, nlp)\n\n\n\n\n\n","category":"method"},{"location":"#Home","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"","category":"section"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"This package provides a few optimization solvers curated by the JuliaSmoothOptimizers organization.","category":"page"},{"location":"#Basic-usage","page":"JSOSolvers.jl documentation","title":"Basic usage","text":"","category":"section"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"All solvers here are JSO-Compliant, in the sense that they accept NLPModels and return GenericExecutionStats. This allows benchmark them easily.","category":"page"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"All solvers can be called like the following:","category":"page"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"stats = solver_name(nlp; kwargs...)","category":"page"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"where nlp is an AbstractNLPModel or some specialization, such as an AbstractNLSModel, and the following keyword arguments are supported:","category":"page"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"x is the starting default (default: nlp.meta.x0);\natol is the absolute stopping tolerance (default: atol = √ϵ);\nrtol is the relative stopping tolerance (default: rtol = √ϵ);\nmax_eval is the maximum number of objective and constraints function evaluations (default: -1, which means no limit);\nmax_time is the maximum allowed elapsed time (default: 30.0);\nstats is a SolverTools.GenericExecutionStats with the output of the solver.","category":"page"},{"location":"","page":"JSOSolvers.jl documentation","title":"JSOSolvers.jl documentation","text":"See the full list of Solvers.","category":"page"}]
}
