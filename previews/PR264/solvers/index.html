<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solvers · JSOSolvers.jl</title><meta name="title" content="Solvers · JSOSolvers.jl"/><meta property="og:title" content="Solvers · JSOSolvers.jl"/><meta property="twitter:title" content="Solvers · JSOSolvers.jl"/><meta name="description" content="Documentation for JSOSolvers.jl."/><meta property="og:description" content="Documentation for JSOSolvers.jl."/><meta property="twitter:description" content="Documentation for JSOSolvers.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="JSOSolvers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">JSOSolvers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">JSOSolvers.jl documentation</a></li><li class="is-active"><a class="tocitem" href>Solvers</a><ul class="internal"><li><a class="tocitem" href="#Solver-list"><span>Solver list</span></a></li></ul></li><li><a class="tocitem" href="../internal/">Internal functions</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/main/docs/src/solvers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Solvers"><a class="docs-heading-anchor" href="#Solvers">Solvers</a><a id="Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Solvers" title="Permalink"></a></h1><p><strong>Solver list</strong></p><ul><li><a href="#JSOSolvers.lbfgs"><code>lbfgs</code></a></li><li><a href="#JSOSolvers.tron"><code>tron</code></a></li><li><a href="#JSOSolvers.trunk"><code>trunk</code></a></li><li><a href="#JSOSolvers.R2"><code>R2</code></a></li><li><a href="#JSOSolvers.fomo"><code>fomo</code></a></li></ul><table><tr><th style="text-align: right">Problem type</th><th style="text-align: right">Solvers</th></tr><tr><td style="text-align: right">Unconstrained NLP</td><td style="text-align: right"><a href="#JSOSolvers.lbfgs"><code>lbfgs</code></a>, <a href="#JSOSolvers.tron"><code>tron</code></a>, <a href="#JSOSolvers.trunk"><code>trunk</code></a>, <a href="#JSOSolvers.R2"><code>R2</code></a>, <a href="#JSOSolvers.fomo"><code>fomo</code></a></td></tr><tr><td style="text-align: right">Unconstrained NLS</td><td style="text-align: right"><a href="#JSOSolvers.trunk"><code>trunk</code></a>, <a href="#JSOSolvers.tron"><code>tron</code></a></td></tr><tr><td style="text-align: right">Bound-constrained NLP</td><td style="text-align: right"><a href="#JSOSolvers.tron"><code>tron</code></a></td></tr><tr><td style="text-align: right">Bound-constrained NLS</td><td style="text-align: right"><a href="#JSOSolvers.tron"><code>tron</code></a></td></tr></table><h2 id="Solver-list"><a class="docs-heading-anchor" href="#Solver-list">Solver list</a><a id="Solver-list-1"></a><a class="docs-heading-anchor-permalink" href="#Solver-list" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JSOSolvers.lbfgs" href="#JSOSolvers.lbfgs"><code>JSOSolvers.lbfgs</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lbfgs(nlp; kwargs...)</code></pre><p>An implementation of a limited memory BFGS line-search method for unconstrained minimization.</p><p>For advanced usage, first define a <code>LBFGSSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>.</p><pre><code class="nohighlight hljs">solver = LBFGSSolver(nlp; mem::Int = 5)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>mem::Int = 5</code>: memory parameter of the <code>lbfgs</code> algorithm.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>τ₁::T = T(0.9999)</code>: slope factor in the Wolfe condition when performing the line search.</li><li><code>bk_max:: Int = 25</code>: maximum number of backtracks when performing the line search.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>verbose_subsolver::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose_subsolver</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has found a stopping criteria. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
stats = lbfgs(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3));
solver = LBFGSSolver(nlp; mem = 5);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/lbfgs.jl#L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JSOSolvers.tron" href="#JSOSolvers.tron"><code>JSOSolvers.tron</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">tron(nlp; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained optimization:</p><pre><code class="nohighlight hljs">    min f(x)    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = TronSolver(nlp; kwargs...)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem&#39;s iteration limit.</li><li><code>use_only_objgrad::Bool = false</code>: If <code>true</code>, the algorithm uses only the function <code>objgrad</code> instead of <code>obj</code> and <code>grad</code>.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolver</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>TRON is described in</p><pre><code class="nohighlight hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
stats = tron(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x), ones(3), zeros(3), 2 * ones(3));
solver = TronSolver(nlp);
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/tron.jl#L177">source</a></section><section><div><pre><code class="language-julia hljs">tron(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for bound-constrained nonlinear least-squares problems:</p><pre><code class="nohighlight hljs">min ½‖F(x)‖²    s.t.    ℓ ≦ x ≦ u</code></pre><p>For advanced usage, first define a <code>TronSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:     solver = TronSolverNLS(nls, subsolver_type::Type{&lt;:KrylovSolver} = LsmrSolver; kwargs...)     solve!(solver, nls; kwargs...)</p><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>subsolver_type::Symbol = LsmrSolver</code>: <code>Krylov.jl</code> method used as subproblem solver, see <code>JSOSolvers.tronls_allowed_subsolvers</code> for a list.</li><li><code>μ₀::T = T(1e-2)</code>: algorithm parameter in (0, 0.5).</li><li><code>μ₁::T = one(T)</code>: algorithm parameter in (0, +∞).</li><li><code>σ::T = T(10)</code>: algorithm parameter in (1, +∞).</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>max_cgiter::Int = 50</code>: subproblem iteration limit.</li><li><code>cgtol::T = T(0.1)</code>: subproblem tolerance.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>The keyword arguments of <code>TronSolverNLS</code> are passed to the <a href="https://github.com/JuliaSmoothOptimizers/SolverTools.jl/blob/main/src/trust-region/tron-trust-region.jl"><code>TRONTrustRegion</code></a> constructor.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This is an adaptation for bound-constrained nonlinear least-squares problems of the TRON method described in</p><pre><code class="nohighlight hljs">Chih-Jen Lin and Jorge J. Moré, *Newton&#39;s Method for Large Bound-Constrained
Optimization Problems*, SIAM J. Optim., 9(4), 1100–1127, 1999.
DOI: 10.1137/S1052623498345075</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
stats = tron(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2, zeros(2), 0.5 * ones(2))
solver = TronSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/tronls.jl#L194">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JSOSolvers.trunk" href="#JSOSolvers.trunk"><code>JSOSolvers.trunk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trunk(nlp; kwargs...)</code></pre><p>A trust-region solver for unconstrained optimization using exact second derivatives.</p><p>For advanced usage, first define a <code>TrunkSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = TrunkSolver(nlp, subsolver_type::Type{&lt;:KrylovSolver} = CgSolver)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>subsolver_logger::AbstractLogger = NullLogger()</code>: subproblem&#39;s logger.</li><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p><strong>Output</strong></p><p>The returned value is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="nohighlight hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = trunk(nlp)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = TrunkSolver(nlp)
stats = solve!(solver, nlp)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/trunk.jl#L134">source</a></section><section><div><pre><code class="language-julia hljs">trunk(nls; kwargs...)</code></pre><p>A pure Julia implementation of a trust-region solver for nonlinear least-squares problems:</p><pre><code class="nohighlight hljs">min ½‖F(x)‖²</code></pre><p>For advanced usage, first define a <code>TrunkSolverNLS</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = TrunkSolverNLS(nls, subsolver_type::Type{&lt;:KrylovSolver} = LsmrSolver)
solve!(solver, nls; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nls::AbstractNLSModel{T, V}</code> represents the model to solve, see <code>NLPModels.jl</code>.</li></ul><p>The keyword arguments may include</p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance, the algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>Fatol::T = √eps(T)</code>: absolute tolerance on the residual.</li><li><code>Frtol::T = eps(T)</code>: relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.</li><li><code>max_eval::Int = -1</code>: maximum number of objective function evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>bk_max::Int = 10</code>: algorithm parameter.</li><li><code>monotone::Bool = true</code>: algorithm parameter.</li><li><code>nm_itmax::Int = 25</code>: algorithm parameter.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>subsolver_verbose::Int = 0</code>: if &gt; 0, display iteration information every <code>subsolver_verbose</code> iteration of the subsolver.</li></ul><p>See <code>JSOSolvers.trunkls_allowed_subsolvers</code> for a list of available <code>KrylovSolver</code>.</p><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>References</strong></p><p>This implementation follows the description given in</p><pre><code class="nohighlight hljs">A. R. Conn, N. I. M. Gould, and Ph. L. Toint,
Trust-Region Methods, volume 1 of MPS/SIAM Series on Optimization.
SIAM, Philadelphia, USA, 2000.
DOI: 10.1137/1.9780898719857</code></pre><p>The main algorithm follows the basic trust-region method described in Section 6. The backtracking linesearch follows Section 10.3.2. The nonmonotone strategy follows Section 10.1.3, Algorithm 10.1.2.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
stats = trunk(nls)</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2)
solver = TrunkSolverNLS(nls)
stats = solve!(solver, nls)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/trunkls.jl#L155">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JSOSolvers.R2" href="#JSOSolvers.R2"><code>JSOSolvers.R2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fo(nlp; kwargs...)
R2(nlp; kwargs...)
TR(nlp; kwargs...)</code></pre><p>A First-Order (FO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = FoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><code>R2</code> and <code>TR</code> runs <code>fo</code> with the dedicated <code>step_backend</code> keyword argument.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1/4)</code>, <code>η2 = T(0.95)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = T(2)</code>: regularization update parameters.</li><li><code>αmax = 1/eps(T)</code>: maximum step parameter for fomo algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of evaluation of the objective function.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: step computation mode. Options are <code>r2_step()</code> for quadratic regulation step and <code>tr_step()</code> for first-order trust-region.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user || stats.stats = :unknown</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fo(nlp) # run with step_backend = r2_step(), equivalent to R2(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/fomo.jl#L241">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="JSOSolvers.fomo" href="#JSOSolvers.fomo"><code>JSOSolvers.fomo</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fomo(nlp; kwargs...)</code></pre><p>A First-Order with MOmentum (FOMO) model-based method for unconstrained optimization. Supports quadratic regularization and trust region method with linear model.</p><p><strong>Algorithm description</strong></p><p>The step is computed along d = - (1-βmax) .* ∇f(xk) - βmax .* mk with mk the memory of past gradients (initialized at 0), and updated at each successful iteration as mk .= ∇f(xk) .* (1 - βmax) .+ mk .* βmax and βmax ∈ [0,β] chosen as to ensure d is gradient-related, i.e., the following 2 conditions are satisfied: (1-βmax) .* ∇f(xk) + βmax .* ∇f(xk)ᵀmk ≥ θ1 * ‖∇f(xk)‖² (1) ‖∇f(xk)‖ ≥ θ2 * ‖(1-βmax) <em>. ∇f(xk) + βmax .</em> mk‖       (2)</p><p><strong>Advanced usage</strong></p><p>For advanced usage, first define a <code>FomoSolver</code> to preallocate the memory used in the algorithm, and then call <code>solve!</code>:</p><pre><code class="nohighlight hljs">solver = FomoSolver(nlp)
solve!(solver, nlp; kwargs...)</code></pre><p><strong>No momentum</strong>: if the user does not whish to use momentum (<code>β</code> = 0), it is recommended to use the memory-optimized <code>fo</code> method.</p><p><strong>Arguments</strong></p><ul><li><code>nlp::AbstractNLPModel{T, V}</code> is the model to solve, see <code>NLPModels.jl</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>x::V = nlp.meta.x0</code>: the initial guess.</li><li><code>atol::T = √eps(T)</code>: absolute tolerance.</li><li><code>rtol::T = √eps(T)</code>: relative tolerance: algorithm stops when ‖∇f(xᵏ)‖ ≤ atol + rtol * ‖∇f(x⁰)‖.</li><li><code>η1 = eps(T)^(1/4)</code>, <code>η2 = T(0.95)</code>: step acceptance parameters.</li><li><code>γ1 = T(1/2)</code>, <code>γ2 = T(2)</code>: regularization update parameters.</li><li><code>γ3 = T(1/2)</code> : momentum factor βmax update parameter in case of unsuccessful iteration.</li><li><code>αmax = 1/eps(T)</code>: maximum step parameter for fomo algorithm.</li><li><code>max_eval::Int = -1</code>: maximum number of objective evaluations.</li><li><code>max_time::Float64 = 30.0</code>: maximum time limit in seconds.</li><li><code>max_iter::Int = typemax(Int)</code>: maximum number of iterations.</li><li><code>β = T(0.9) ∈ [0,1)</code>: target decay rate for the momentum.</li><li><code>θ1 = T(0.1)</code>: momentum contribution parameter for convergence condition (1).</li><li><code>θ2 = T(eps(T)^(1/3))</code>: momentum contribution parameter for convergence condition (2). </li><li><code>verbose::Int = 0</code>: if &gt; 0, display iteration details every <code>verbose</code> iteration.</li><li><code>step_backend = r2_step()</code>: step computation mode. Options are <code>r2_step()</code> for quadratic regulation step and <code>tr_step()</code> for first-order trust-region.</li></ul><p><strong>Output</strong></p><p>The value returned is a <code>GenericExecutionStats</code>, see <code>SolverCore.jl</code>.</p><p><strong>Callback</strong></p><p>The callback is called at each iteration. The expected signature of the callback is <code>callback(nlp, solver, stats)</code>, and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting <code>stats.status = :user || stats.stats = :unknown</code> will stop the algorithm. All relevant information should be available in <code>nlp</code> and <code>solver</code>. Notably, you can access, and modify, the following:</p><ul><li><code>solver.x</code>: current iterate;</li><li><code>solver.gx</code>: current gradient;</li><li><code>stats</code>: structure holding the output of the algorithm (<code>GenericExecutionStats</code>), which contains, among other things:<ul><li><code>stats.dual_feas</code>: norm of current gradient;</li><li><code>stats.iter</code>: current iteration counter;</li><li><code>stats.objective</code>: current objective function value;</li><li><code>stats.status</code>: current status of the algorithm. Should be <code>:unknown</code> unless the algorithm has attained a stopping criterion. Changing this to anything will stop the algorithm, but you should use <code>:user</code> to properly indicate the intention.</li><li><code>stats.elapsed_time</code>: elapsed time in seconds.</li></ul></li></ul><p><strong>Examples</strong></p><p><strong><code>fomo</code></strong></p><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
stats = fomo(nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre><pre><code class="language-julia hljs">using JSOSolvers, ADNLPModels
nlp = ADNLPModel(x -&gt; sum(x.^2), ones(3))
solver = FomoSolver(nlp);
stats = solve!(solver, nlp)

# output

&quot;Execution stats: first-order stationary&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/5047fbddd50044f0288d9e614e5d1605f7237469/src/fomo.jl#L121">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« JSOSolvers.jl documentation</a><a class="docs-footer-nextpage" href="../internal/">Internal functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 22 March 2024 11:31">Friday 22 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
